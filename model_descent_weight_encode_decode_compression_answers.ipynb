{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "Rlc2iOtbNxMB",
   "metadata": {
    "id": "Rlc2iOtbNxMB"
   },
   "outputs": [],
   "source": [
    "#!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "329a74b6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "329a74b6",
    "outputId": "3ebe770e-e155-4d9a-a169-ef547a9e14de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "## Please install torch and datasets\n",
    "import torch\n",
    "from torchvision.transforms import functional as t\n",
    "import torch.nn.functional as f\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0934cf3a",
   "metadata": {
    "id": "0934cf3a"
   },
   "outputs": [],
   "source": [
    "## Loading our dataset\n",
    "ds = load_dataset(\"ylecun/mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb5cbf19",
   "metadata": {
    "id": "fb5cbf19"
   },
   "outputs": [],
   "source": [
    "## Data splits\n",
    "\n",
    "X_train_p = ds[\"train\"][\"image\"]\n",
    "Y_train = ds[\"train\"][\"label\"]\n",
    "X_test_p = ds[\"test\"][\"image\"]\n",
    "Y_test = ds[\"test\"][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d0c8b78",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3d0c8b78",
    "outputId": "fa7568ff-42b2-488a-c914-37d068da6707"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 1, 28, 28]) torch.Size([10000, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "## PIL to Tensors\n",
    "\n",
    "X_train = [t.pil_to_tensor(x) for x in X_train_p]\n",
    "X_test = [t.pil_to_tensor(x) for x in X_test_p]\n",
    "X_train = torch.stack(X_train).to(device)\n",
    "X_test = torch.stack(X_test).to(device)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d32ba99f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d32ba99f",
    "outputId": "d6b9fdd1-b141-4d0c-f63c-eb1833ecbaa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28]) torch.Size([10000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "## Fixing the shape\n",
    "\n",
    "X_train = X_train.view(-1, 28, 28)\n",
    "X_test = X_test.view(-1, 28, 28)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34acb755",
   "metadata": {
    "id": "34acb755"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000]) torch.Size([10000]) torch.Size([60000, 10])\n"
     ]
    }
   ],
   "source": [
    "## Making labels into tensors\n",
    "\n",
    "Y_train = torch.tensor(Y_train).to(device)\n",
    "Y_test = torch.tensor(Y_test).to(device)\n",
    "Y_train_onehot = f.one_hot(Y_train, num_classes=10).float()\n",
    "print(Y_train.shape, Y_test.shape, Y_train_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5239aee9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5239aee9",
    "outputId": "a7b8d46a-c476-4704-b77e-c3344066ccca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 784]) torch.Size([10000, 784])\n"
     ]
    }
   ],
   "source": [
    "## Flattening the image as DNN takes flat tensor as input\n",
    "\n",
    "X_train = X_train.view(-1, 784).float() / 255.0\n",
    "X_test = X_test.view(-1, 784).float() / 255.0\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sGEKSOBrVZxy",
   "metadata": {
    "id": "sGEKSOBrVZxy"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import svd\n",
    "\n",
    "\n",
    "\n",
    "class Linear():\n",
    "  def __init__(self, input_dims, output_dims, B=True, last=False):\n",
    "    self.training = True\n",
    "    self.W = (torch.randn(input_dims, output_dims) * (5/3) / (input_dims**0.5)).to(device) if not last else (torch.randn(input_dims, output_dims) * (5/3) / (input_dims**0.5) * 0.1).to(device)\n",
    "    if B: self.B = torch.randn(output_dims).to(device) if not last else (torch.randn(output_dims) * 0.1).to(device)\n",
    "    else: self.B = torch.tensor([]).to(device)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    if not torch.equal(self.B, torch.tensor([]).to(device)): self.result = x@self.W + self.B\n",
    "    else: self.result = x@self.W\n",
    "    return self.result\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.W] + [self.B]\n",
    "\n",
    "\n",
    "class Tanh():\n",
    "  def __init__(self):\n",
    "    self.training = True\n",
    "    return None\n",
    "\n",
    "  def __call__(self, x):\n",
    "    self.result = torch.tanh(x)\n",
    "    return self.result\n",
    "\n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "class Dropout():\n",
    "  def __init__(self, batch_size, output_dims, rate=0.9):\n",
    "    self.training = True\n",
    "    self.rate = rate\n",
    "    self.factor = (torch.rand(batch_size, output_dims) < self.rate).int().to(device)\n",
    "    return None\n",
    "\n",
    "  def __call__(self, x):\n",
    "    if self.training: self.result = x * self.factor\n",
    "    else: self.result = x\n",
    "    return self.result\n",
    "\n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "\n",
    "def decomposition(A, k=1):\n",
    "\n",
    "    # SVD\n",
    "    U, S, VT = svd(A, full_matrices=False)\n",
    "\n",
    "\n",
    "    # Truncate to rank-k\n",
    "    U_k = U[:, :k]                # (784 x k)\n",
    "    S_k = np.diag(S[:k])          # (k x k)\n",
    "    VT_k = VT[:k, :]              # (k x 10)\n",
    "\n",
    "    # Factor A_k = L @ R, where L and R are low-rank factors\n",
    "    sqrt_S_k = np.sqrt(S_k)       # (k x k)\n",
    "    L = U_k @ sqrt_S_k            # (784 x k)\n",
    "    R = sqrt_S_k @ VT_k           # (k x 10)\n",
    "\n",
    "    L_flat = L.flatten()\n",
    "    R_flat = R.flatten()\n",
    "    LR_concat = np.concatenate([L_flat, R_flat])\n",
    "\n",
    "    return LR_concat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "PK1PLMxCWaM9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PK1PLMxCWaM9",
    "outputId": "b2943435-c8dd-4304-a8d2-8d1c5dece336"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updater parameters: 61544000, Predictor parameters: 7840\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "\n",
    "mid_dims = 3920\n",
    "\n",
    "W_u1 = (torch.randn(7860, mid_dims) * (5/3) / (7840**0.5)).to(device)\n",
    "W_u2 = (torch.randn(mid_dims, 7840) * (5/3) / (mid_dims**0.5)).to(device)\n",
    "W_p_t_1 = (torch.randn(784, 10) * (5/3) / (784**0.5)).to(device)\n",
    "\n",
    "W_u1.requires_grad = True\n",
    "W_u2.requires_grad = True\n",
    "W_p_t_1.requires_grad = False\n",
    "\n",
    "updaterparams = [W_u1, W_u2]\n",
    "\n",
    "print(f\"Updater parameters: {W_u1.numel() + W_u2.numel()}, Predictor parameters: {W_p_t_1.numel()}\")\n",
    "\n",
    "\n",
    "Y_p_t_1 = torch.zeros(batch_size, 10).to(device)\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e298304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0, Loss: 2.536762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_548508/2130484074.py:20: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  if W_p_t_1.grad is not None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration   40, Loss: 2.830564\n",
      "Iteration   80, Loss: 2.715928\n",
      "Iteration  120, Loss: 2.690230\n",
      "Iteration  160, Loss: 2.821527\n",
      "Iteration  200, Loss: 2.384098\n",
      "Iteration  240, Loss: 2.500139\n",
      "Iteration  280, Loss: 2.586493\n",
      "Iteration  320, Loss: 2.656035\n",
      "Iteration  360, Loss: 2.622307\n",
      "Iteration  400, Loss: 2.546935\n",
      "Iteration  440, Loss: 2.613420\n",
      "Iteration  480, Loss: 2.800249\n",
      "Iteration  520, Loss: 2.588357\n",
      "Iteration  560, Loss: 2.505977\n",
      "Iteration  600, Loss: 2.238578\n",
      "Iteration  640, Loss: 1.654592\n",
      "Iteration  680, Loss: 1.284471\n",
      "Iteration  720, Loss: 1.219195\n",
      "Iteration  760, Loss: 1.127989\n",
      "Iteration  800, Loss: 1.103439\n",
      "Iteration  840, Loss: 1.073142\n",
      "Iteration  880, Loss: 1.150408\n",
      "Iteration  920, Loss: 1.091032\n",
      "Iteration  960, Loss: 1.138247\n",
      "Iteration 1000, Loss: 1.071779\n",
      "Iteration 1040, Loss: 1.077584\n",
      "Iteration 1080, Loss: 1.097980\n",
      "Iteration 1120, Loss: 1.062259\n",
      "Iteration 1160, Loss: 1.059123\n",
      "Iteration 1200, Loss: 1.072796\n",
      "Iteration 1240, Loss: 1.044479\n",
      "Iteration 1280, Loss: 1.075699\n",
      "Iteration 1320, Loss: 1.036778\n",
      "Iteration 1360, Loss: 1.053109\n",
      "Iteration 1400, Loss: 1.033483\n",
      "Iteration 1440, Loss: 0.981497\n",
      "Iteration 1480, Loss: 1.057428\n",
      "Iteration 1520, Loss: 1.023807\n",
      "Iteration 1560, Loss: 0.988539\n",
      "Iteration 1600, Loss: 1.064366\n",
      "Iteration 1640, Loss: 1.038792\n",
      "Iteration 1680, Loss: 1.036816\n",
      "Iteration 1720, Loss: 0.971246\n",
      "Iteration 1760, Loss: 1.010241\n",
      "Iteration 1800, Loss: 1.020896\n",
      "Iteration 1840, Loss: 1.037188\n",
      "Iteration 1880, Loss: 1.061574\n",
      "Iteration 1920, Loss: 1.013656\n",
      "Iteration 1960, Loss: 1.044228\n",
      "Iteration 2000, Loss: 1.037225\n",
      "Iteration 2040, Loss: 1.021873\n",
      "Iteration 2080, Loss: 1.041959\n",
      "Iteration 2120, Loss: 0.988665\n",
      "Iteration 2160, Loss: 0.980349\n",
      "Iteration 2200, Loss: 1.055578\n",
      "Iteration 2240, Loss: 1.021477\n",
      "Iteration 2280, Loss: 1.029806\n",
      "Iteration 2320, Loss: 1.046811\n",
      "Iteration 2360, Loss: 1.014030\n",
      "Iteration 2400, Loss: 1.001147\n",
      "Iteration 2440, Loss: 1.049568\n",
      "Iteration 2480, Loss: 1.047470\n",
      "Iteration 2520, Loss: 0.989987\n",
      "Iteration 2560, Loss: 0.989068\n",
      "Iteration 2600, Loss: 1.055559\n",
      "Iteration 2640, Loss: 1.001240\n",
      "Iteration 2680, Loss: 0.961875\n",
      "Iteration 2720, Loss: 1.010522\n",
      "Iteration 2760, Loss: 1.025499\n",
      "Iteration 2800, Loss: 1.053044\n",
      "Iteration 2840, Loss: 0.994824\n",
      "Iteration 2880, Loss: 1.021188\n",
      "Iteration 2920, Loss: 1.015584\n",
      "Iteration 2960, Loss: 0.957816\n",
      "Iteration 3000, Loss: 1.024398\n",
      "Iteration 3040, Loss: 1.040396\n",
      "Iteration 3080, Loss: 1.039048\n",
      "Iteration 3120, Loss: 1.029967\n",
      "Iteration 3160, Loss: 1.014479\n",
      "Iteration 3200, Loss: 0.972303\n",
      "Iteration 3240, Loss: 1.025432\n",
      "Iteration 3280, Loss: 1.057432\n",
      "Iteration 3320, Loss: 0.949623\n",
      "Iteration 3360, Loss: 0.978781\n",
      "Iteration 3400, Loss: 0.983943\n",
      "Iteration 3440, Loss: 1.015313\n",
      "Iteration 3480, Loss: 0.970281\n",
      "Iteration 3520, Loss: 0.982754\n",
      "Iteration 3560, Loss: 1.005100\n",
      "Iteration 3600, Loss: 0.989506\n",
      "Iteration 3640, Loss: 1.027988\n",
      "Iteration 3680, Loss: 0.955619\n",
      "Iteration 3720, Loss: 0.978566\n",
      "Iteration 3760, Loss: 1.011117\n",
      "Iteration 3800, Loss: 1.002665\n",
      "Iteration 3840, Loss: 0.979565\n",
      "Iteration 3880, Loss: 0.998076\n",
      "Iteration 3920, Loss: 1.053697\n",
      "Iteration 3960, Loss: 1.037104\n"
     ]
    }
   ],
   "source": [
    "## Training loop for updater and predictor\n",
    "iters = 4000\n",
    "alpha = 0.008\n",
    "\n",
    "\n",
    "for c in range(iters):\n",
    "    # Pick a random batch for this iteration\n",
    "    batch_indices = torch.randperm(X_train.size(0))[:batch_size]\n",
    "    X_batch = X_train[batch_indices]\n",
    "    Y_batch_onehot = Y_train_onehot[batch_indices]\n",
    "    Y_batch = Y_train[batch_indices]\n",
    "    #print(X_batch.shape, Y_batch_onehot.shape, Y_batch.shape, flat_W_p_t_detached.unsqueeze(0).expand(batch_size, 7840).shape, Y_batch.detach().shape, Y_batch_onehot.detach().shape)\n",
    "\n",
    "    # Zero updater gradients\n",
    "    for p in updaterparams:\n",
    "        if p.grad is not None:\n",
    "            p.grad.zero_()\n",
    "\n",
    "    # Zero predictor gradients\n",
    "    if W_p_t_1.grad is not None:\n",
    "        W_p_t_1.grad.zero_()\n",
    "\n",
    "    W_p_t_detached = W_p_t_1.detach()\n",
    "    flat_W_p_t_detached = W_p_t_detached.view(-1)\n",
    "    x = torch.cat((flat_W_p_t_detached.unsqueeze(0).expand(batch_size, 7840), Y_p_t_1.detach(), Y_batch_onehot.detach()), dim=1)\n",
    "    x = torch.tanh(x @ W_u1)\n",
    "    flat_W_p_t_1 = torch.tanh(x @ W_u2)\n",
    "    flat_W_p_t_1 = flat_W_p_t_1.mean(dim=0)  # Average over the batch\n",
    "    W_p_t_1 = flat_W_p_t_1.view(784, 10)\n",
    "\n",
    "    Y_p_t_1 = torch.tanh(X_batch @ W_p_t_1)\n",
    "    loss = f.cross_entropy(Y_p_t_1, Y_batch)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    for p in updaterparams:\n",
    "        p.data -= alpha * p.grad\n",
    "\n",
    "    if c % (iters // 100) == 0:\n",
    "        print(f\"Iteration {c:4d}, Loss: {loss.item():.6f}\")\n",
    "        losses.append(loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae3b0081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Training loop for updater and predictor with Adam optimizer\n",
    "# iters = 4000\n",
    "# alpha = 0.008\n",
    "\n",
    "# # Create Adam optimizer for updater parameters\n",
    "# adam_optimizer = torch.optim.Adam(updaterparams, lr=alpha)\n",
    "\n",
    "# for c in range(iters):\n",
    "#     # Zero updater gradients\n",
    "#     adam_optimizer.zero_grad()\n",
    "\n",
    "#     # Zero predictor gradients\n",
    "#     if W_p_t_1.grad is not None:\n",
    "#         W_p_t_1.grad.zero_()\n",
    "\n",
    "#     W_p_t_detached = W_p_t_1.detach()\n",
    "#     flat_W_p_t_detached = W_p_t_detached.view(-1)\n",
    "#     x = torch.cat((flat_W_p_t_detached.unsqueeze(0).expand(batch_size, 7840), Y_p_t_1.detach(), Y_train_onehot.detach()), dim=1)\n",
    "#     x = torch.tanh(x @ W_u1)\n",
    "#     flat_W_p_t_1 = torch.tanh(x @ W_u2)\n",
    "#     flat_W_p_t_1 = flat_W_p_t_1.mean(dim=0)  # Average over the batch\n",
    "#     W_p_t_1 = flat_W_p_t_1.view(784, 10)\n",
    "\n",
    "#     Y_p_t_1 = torch.tanh(X_train @ W_p_t_1)\n",
    "#     loss = f.cross_entropy(Y_p_t_1, Y_train)\n",
    "\n",
    "#     loss.backward()\n",
    "\n",
    "#     # Use Adam optimizer step instead of manual SGD\n",
    "#     adam_optimizer.step()\n",
    "\n",
    "#     if c % (iters // 1000) == 0:\n",
    "#         print(f\"Iteration {c:4d}, Loss: {loss.item():.6f}\")\n",
    "#         losses.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf5fb936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 89.30166666666666 | test accuracy: 89.64\n",
      "train loss: 1.0046555995941162 | test loss: 1.005833387374878\n"
     ]
    }
   ],
   "source": [
    "def accuracy(X, Y):\n",
    "    x = torch.tanh(X @ W_p_t_1)\n",
    "    probs = f.softmax(x, 1)\n",
    "    answers = x.argmax(1)\n",
    "    c = 0\n",
    "    for a, y in zip(answers, Y):\n",
    "        if a==y: c+=1\n",
    "    return c / answers.shape[0] * 100\n",
    "\n",
    "def loss(X, Y):\n",
    "    x = torch.tanh(X @ W_p_t_1)\n",
    "    return f.cross_entropy(x, Y)\n",
    "\n",
    "print(f\"train accuracy: {accuracy(X_train, Y_train)} | test accuracy: {accuracy(X_test, Y_test)}\")\n",
    "print(f\"train loss: {loss(X_train, Y_train)} | test loss: {loss(X_test, Y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7209328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# plt.figure(figsize=(8, 4))\n",
    "# plt.plot(losses)\n",
    "# plt.xlabel('Iteration')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Training Loss')\n",
    "\n",
    "# # Get current notebook filename and save as PNG\n",
    "# notebook_path = 'model_descent_weight_enc_dec_compression_3920_4000it'\n",
    "# filename = os.path.splitext(notebook_path)[0] + '.png'\n",
    "# plt.savefig(filename)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b349e18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 60000\n",
    "# #loss1iters = []\n",
    "\n",
    "\n",
    "# for midf in range(24, 34):\n",
    "#     mid_dims = (midf+1) * 196\n",
    "#     W_u1 = (torch.randn(7840, mid_dims) * (5/3) / (7840**0.5)).to(device)\n",
    "#     W_u2 = (torch.randn(mid_dims, 7840) * (5/3) / (mid_dims**0.5)).to(device)\n",
    "#     W_p_t_1 = (torch.randn(784, 10) * (5/3) / (784**0.5)).to(device)\n",
    "\n",
    "#     W_u1.requires_grad = True\n",
    "#     W_u2.requires_grad = True\n",
    "#     W_p_t_1.requires_grad = False\n",
    "\n",
    "#     updaterparams = [W_u1, W_u2]\n",
    "\n",
    "#     print(f\"Updater parameters: {W_u1.numel() + W_u2.numel()}, Predictor parameters: {W_p_t_1.numel()}\")\n",
    "\n",
    "#     losses = []\n",
    "\n",
    "#     ## Training loop for updater and predictor\n",
    "#     alpha = 0.008\n",
    "\n",
    "#     c = -1\n",
    "#     while True:\n",
    "#         c += 1\n",
    "\n",
    "#         # Zero updater gradients\n",
    "#         for p in updaterparams:\n",
    "#             if p.grad is not None:\n",
    "#                 p.grad.zero_()\n",
    "\n",
    "#         # Zero predictor gradients\n",
    "#         if W_p_t_1.grad is not None:\n",
    "#             W_p_t_1.grad.zero_()\n",
    "\n",
    "#         W_p_t_detached = W_p_t_1.detach()\n",
    "#         flat_W_p_t_detached = W_p_t_detached.view(-1)\n",
    "#         x = torch.tanh(flat_W_p_t_detached @ W_u1)\n",
    "#         flat_W_p_t_1 = torch.tanh(x @ W_u2)\n",
    "#         W_p_t_1 = flat_W_p_t_1.view(784, 10)\n",
    "\n",
    "#         Y_p_t_1 = torch.tanh(X_train @ W_p_t_1)\n",
    "#         loss = f.cross_entropy(Y_p_t_1, Y_train)\n",
    "\n",
    "#         loss.backward()\n",
    "\n",
    "#         for p in updaterparams:\n",
    "#             p.data -= alpha * p.grad\n",
    "\n",
    "#         if c>6000: \n",
    "#             print(\"Couldn't find a solution in 6000 iterations\")\n",
    "#             break\n",
    "#         if loss.item() < 1.05:\n",
    "#             print(\"Found a solution in\", c, \"iterations with mid_dims =\", mid_dims)\n",
    "#             loss1iters.append([mid_dims, c])\n",
    "#             break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "701721ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss1iters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Plotting loss1iters with (midf+1)*392 on x-axis and c on y-axis\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mloss1iters\u001b[49m:\n\u001b[1;32m      5\u001b[0m     loss1iters_arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(loss1iters)\n\u001b[1;32m      6\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m4\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss1iters' is not defined"
     ]
    }
   ],
   "source": [
    "# Plotting loss1iters with (midf+1)*392 on x-axis and c on y-axis\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if loss1iters:\n",
    "    loss1iters_arr = np.array(loss1iters)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(loss1iters_arr[:,0], loss1iters_arr[:,1], marker='o')\n",
    "    plt.xlabel('Mid Dims')\n",
    "    plt.ylabel('Iterations to reach loss < 1.0')\n",
    "    plt.title('Iterations vs. Hidden Layer Size')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"loss1iters is empty.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
