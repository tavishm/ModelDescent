{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "Rlc2iOtbNxMB",
   "metadata": {
    "id": "Rlc2iOtbNxMB"
   },
   "outputs": [],
   "source": [
    "#!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "329a74b6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "329a74b6",
    "outputId": "3ebe770e-e155-4d9a-a169-ef547a9e14de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "## Please install torch and datasets\n",
    "import torch\n",
    "from torchvision.transforms import functional as t\n",
    "import torch.nn.functional as f\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0934cf3a",
   "metadata": {
    "id": "0934cf3a"
   },
   "outputs": [],
   "source": [
    "## Loading our dataset\n",
    "ds = load_dataset(\"ylecun/mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb5cbf19",
   "metadata": {
    "id": "fb5cbf19"
   },
   "outputs": [],
   "source": [
    "## Data splits\n",
    "\n",
    "X_train_p = ds[\"train\"][\"image\"]\n",
    "Y_train = ds[\"train\"][\"label\"]\n",
    "X_test_p = ds[\"test\"][\"image\"]\n",
    "Y_test = ds[\"test\"][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d0c8b78",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3d0c8b78",
    "outputId": "fa7568ff-42b2-488a-c914-37d068da6707"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 1, 28, 28]) torch.Size([10000, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "## PIL to Tensors\n",
    "\n",
    "X_train = [t.pil_to_tensor(x) for x in X_train_p]\n",
    "X_test = [t.pil_to_tensor(x) for x in X_test_p]\n",
    "X_train = torch.stack(X_train).to(device)\n",
    "X_test = torch.stack(X_test).to(device)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d32ba99f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d32ba99f",
    "outputId": "d6b9fdd1-b141-4d0c-f63c-eb1833ecbaa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28]) torch.Size([10000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "## Fixing the shape\n",
    "\n",
    "X_train = X_train.view(-1, 28, 28)\n",
    "X_test = X_test.view(-1, 28, 28)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34acb755",
   "metadata": {
    "id": "34acb755"
   },
   "outputs": [],
   "source": [
    "## Making labels into tensors\n",
    "\n",
    "Y_train = torch.tensor(Y_train).to(device)\n",
    "Y_test = torch.tensor(Y_test).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5239aee9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5239aee9",
    "outputId": "a7b8d46a-c476-4704-b77e-c3344066ccca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 784]) torch.Size([10000, 784])\n"
     ]
    }
   ],
   "source": [
    "## Flattening the image as DNN takes flat tensor as input\n",
    "\n",
    "X_train = X_train.view(-1, 784).float() / 255.0\n",
    "X_test = X_test.view(-1, 784).float() / 255.0\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sGEKSOBrVZxy",
   "metadata": {
    "id": "sGEKSOBrVZxy"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import svd\n",
    "\n",
    "\n",
    "\n",
    "class Linear():\n",
    "  def __init__(self, input_dims, output_dims, B=True, last=False):\n",
    "    self.training = True\n",
    "    self.W = (torch.randn(input_dims, output_dims) * (5/3) / (input_dims**0.5)).to(device) if not last else (torch.randn(input_dims, output_dims) * (5/3) / (input_dims**0.5) * 0.1).to(device)\n",
    "    if B: self.B = torch.randn(output_dims).to(device) if not last else (torch.randn(output_dims) * 0.1).to(device)\n",
    "    else: self.B = torch.tensor([]).to(device)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    if not torch.equal(self.B, torch.tensor([]).to(device)): self.result = x@self.W + self.B\n",
    "    else: self.result = x@self.W\n",
    "    return self.result\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.W] + [self.B]\n",
    "\n",
    "\n",
    "class Tanh():\n",
    "  def __init__(self):\n",
    "    self.training = True\n",
    "    return None\n",
    "\n",
    "  def __call__(self, x):\n",
    "    self.result = torch.tanh(x)\n",
    "    return self.result\n",
    "\n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "class Dropout():\n",
    "  def __init__(self, batch_size, output_dims, rate=0.9):\n",
    "    self.training = True\n",
    "    self.rate = rate\n",
    "    self.factor = (torch.rand(batch_size, output_dims) < self.rate).int().to(device)\n",
    "    return None\n",
    "\n",
    "  def __call__(self, x):\n",
    "    if self.training: self.result = x * self.factor\n",
    "    else: self.result = x\n",
    "    return self.result\n",
    "\n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "\n",
    "def decomposition(A, k=1):\n",
    "\n",
    "    # SVD\n",
    "    U, S, VT = svd(A, full_matrices=False)\n",
    "\n",
    "\n",
    "    # Truncate to rank-k\n",
    "    U_k = U[:, :k]                # (784 x k)\n",
    "    S_k = np.diag(S[:k])          # (k x k)\n",
    "    VT_k = VT[:k, :]              # (k x 10)\n",
    "\n",
    "    # Factor A_k = L @ R, where L and R are low-rank factors\n",
    "    sqrt_S_k = np.sqrt(S_k)       # (k x k)\n",
    "    L = U_k @ sqrt_S_k            # (784 x k)\n",
    "    R = sqrt_S_k @ VT_k           # (k x 10)\n",
    "\n",
    "    L_flat = L.flatten()\n",
    "    R_flat = R.flatten()\n",
    "    LR_concat = np.concatenate([L_flat, R_flat])\n",
    "\n",
    "    return LR_concat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PK1PLMxCWaM9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PK1PLMxCWaM9",
    "outputId": "b2943435-c8dd-4304-a8d2-8d1c5dece336"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1077274\n"
     ]
    }
   ],
   "source": [
    "n1 = 512\n",
    "n2 = 256\n",
    "n3 = 512\n",
    "n4 = 794\n",
    "batch_size = 1\n",
    "\n",
    "updater = [\n",
    "    Linear(794, n1), Tanh(), Dropout(batch_size, n1),\n",
    "    Linear(n1, n2), Tanh(), Dropout(batch_size, n2),\n",
    "    Linear(n2, n3), Tanh(), Dropout(batch_size, n3),\n",
    "   Linear(n3, n4, last=True),\n",
    "    \n",
    "]\n",
    "\n",
    "predictor = [\n",
    "    Linear(784, 10, last=True, B=False)\n",
    "]\n",
    "\n",
    "updater_params = [p for layer in updater for p in layer.parameters()]\n",
    "numparams = 0\n",
    "for p in updater_params:\n",
    "    p.requires_grad = True\n",
    "    numparams += p.numel()\n",
    "print(numparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e298304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0, Loss: 1.703976\n",
      "Iteration   20, Loss: 1.715374\n",
      "Iteration   40, Loss: 1.720156\n",
      "Iteration   60, Loss: 1.703181\n",
      "Iteration   80, Loss: 1.669275\n",
      "Iteration  100, Loss: 1.657724\n",
      "Iteration  120, Loss: 1.662059\n",
      "Iteration  140, Loss: 1.649371\n",
      "Iteration  160, Loss: 1.667636\n",
      "Iteration  180, Loss: 1.689111\n",
      "Iteration  200, Loss: 1.688696\n",
      "Iteration  220, Loss: 1.698792\n",
      "Iteration  240, Loss: 1.687973\n",
      "Iteration  260, Loss: 1.689571\n",
      "Iteration  280, Loss: 1.681423\n",
      "Iteration  300, Loss: 1.682575\n",
      "Iteration  320, Loss: 1.676853\n",
      "Iteration  340, Loss: 1.683933\n",
      "Iteration  360, Loss: 1.686376\n",
      "Iteration  380, Loss: 1.682930\n",
      "Iteration  400, Loss: 1.692330\n",
      "Iteration  420, Loss: 1.690737\n",
      "Iteration  440, Loss: 1.660343\n",
      "Iteration  460, Loss: 1.652849\n",
      "Iteration  480, Loss: 1.653422\n",
      "Iteration  500, Loss: 1.633707\n",
      "Iteration  520, Loss: 1.661278\n",
      "Iteration  540, Loss: 1.638743\n",
      "Iteration  560, Loss: 1.616091\n",
      "Iteration  580, Loss: 1.638188\n",
      "Iteration  600, Loss: 1.644286\n",
      "Iteration  620, Loss: 1.635657\n",
      "Iteration  640, Loss: 1.643222\n",
      "Iteration  660, Loss: 1.634601\n",
      "Iteration  680, Loss: 1.657431\n",
      "Iteration  700, Loss: 1.657733\n",
      "Iteration  720, Loss: 1.646185\n",
      "Iteration  740, Loss: 1.655690\n",
      "Iteration  760, Loss: 1.655342\n",
      "Iteration  780, Loss: 1.644025\n",
      "Iteration  800, Loss: 1.636543\n",
      "Iteration  820, Loss: 1.652214\n",
      "Iteration  840, Loss: 1.659981\n",
      "Iteration  860, Loss: 1.665145\n",
      "Iteration  880, Loss: 1.677043\n",
      "Iteration  900, Loss: 1.673136\n",
      "Iteration  920, Loss: 1.658542\n",
      "Iteration  940, Loss: 1.643984\n",
      "Iteration  960, Loss: 1.634974\n",
      "Iteration  980, Loss: 1.644873\n",
      "Iteration 1000, Loss: 1.651071\n",
      "Iteration 1020, Loss: 1.664443\n",
      "Iteration 1040, Loss: 1.665371\n",
      "Iteration 1060, Loss: 1.661399\n",
      "Iteration 1080, Loss: 1.658425\n",
      "Iteration 1100, Loss: 1.653213\n",
      "Iteration 1120, Loss: 1.652152\n",
      "Iteration 1140, Loss: 1.653807\n",
      "Iteration 1160, Loss: 1.657175\n",
      "Iteration 1180, Loss: 1.661102\n",
      "Iteration 1200, Loss: 1.660522\n",
      "Iteration 1220, Loss: 1.639827\n",
      "Iteration 1240, Loss: 1.630407\n",
      "Iteration 1260, Loss: 1.645960\n",
      "Iteration 1280, Loss: 1.644181\n",
      "Iteration 1300, Loss: 1.632695\n",
      "Iteration 1320, Loss: 1.618746\n",
      "Iteration 1340, Loss: 1.629982\n",
      "Iteration 1360, Loss: 1.638353\n",
      "Iteration 1380, Loss: 1.660164\n",
      "Iteration 1400, Loss: 1.666981\n",
      "Iteration 1420, Loss: 1.661418\n",
      "Iteration 1440, Loss: 1.647555\n",
      "Iteration 1460, Loss: 1.627269\n",
      "Iteration 1480, Loss: 1.624548\n",
      "Iteration 1500, Loss: 1.618515\n",
      "Iteration 1520, Loss: 1.631280\n",
      "Iteration 1540, Loss: 1.636418\n",
      "Iteration 1560, Loss: 1.645416\n",
      "Iteration 1580, Loss: 1.646141\n",
      "Iteration 1600, Loss: 1.641188\n",
      "Iteration 1620, Loss: 1.640835\n",
      "Iteration 1640, Loss: 1.645937\n",
      "Iteration 1660, Loss: 1.650914\n",
      "Iteration 1680, Loss: 1.656529\n",
      "Iteration 1700, Loss: 1.655410\n",
      "Iteration 1720, Loss: 1.646911\n",
      "Iteration 1740, Loss: 1.644938\n",
      "Iteration 1760, Loss: 1.652079\n",
      "Iteration 1780, Loss: 1.656342\n",
      "Iteration 1800, Loss: 1.652349\n",
      "Iteration 1820, Loss: 1.643498\n",
      "Iteration 1840, Loss: 1.642292\n",
      "Iteration 1860, Loss: 1.653729\n",
      "Iteration 1880, Loss: 1.652245\n",
      "Iteration 1900, Loss: 1.641371\n",
      "Iteration 1920, Loss: 1.640586\n",
      "Iteration 1940, Loss: 1.623428\n",
      "Iteration 1960, Loss: 1.622367\n",
      "Iteration 1980, Loss: 1.612500\n",
      "Iteration 2000, Loss: 1.632511\n",
      "Iteration 2020, Loss: 1.649123\n",
      "Iteration 2040, Loss: 1.662463\n",
      "Iteration 2060, Loss: 1.647254\n",
      "Iteration 2080, Loss: 1.643141\n",
      "Iteration 2100, Loss: 1.620814\n",
      "Iteration 2120, Loss: 1.624423\n",
      "Iteration 2140, Loss: 1.625814\n",
      "Iteration 2160, Loss: 1.639992\n",
      "Iteration 2180, Loss: 1.638837\n",
      "Iteration 2200, Loss: 1.641910\n",
      "Iteration 2220, Loss: 1.645718\n",
      "Iteration 2240, Loss: 1.640997\n",
      "Iteration 2260, Loss: 1.636486\n",
      "Iteration 2280, Loss: 1.640604\n",
      "Iteration 2300, Loss: 1.643464\n",
      "Iteration 2320, Loss: 1.641004\n",
      "Iteration 2340, Loss: 1.639552\n",
      "Iteration 2360, Loss: 1.642246\n",
      "Iteration 2380, Loss: 1.646408\n",
      "Iteration 2400, Loss: 1.646613\n",
      "Iteration 2420, Loss: 1.642286\n",
      "Iteration 2440, Loss: 1.642874\n",
      "Iteration 2460, Loss: 1.648180\n",
      "Iteration 2480, Loss: 1.644791\n",
      "Iteration 2500, Loss: 1.636853\n",
      "Iteration 2520, Loss: 1.643231\n",
      "Iteration 2540, Loss: 1.640087\n",
      "Iteration 2560, Loss: 1.641743\n",
      "Iteration 2580, Loss: 1.635891\n",
      "Iteration 2600, Loss: 1.636183\n",
      "Iteration 2620, Loss: 1.626196\n",
      "Iteration 2640, Loss: 1.629597\n",
      "Iteration 2660, Loss: 1.629364\n",
      "Iteration 2680, Loss: 1.635672\n",
      "Iteration 2700, Loss: 1.625916\n",
      "Iteration 2720, Loss: 1.621106\n",
      "Iteration 2740, Loss: 1.619198\n",
      "Iteration 2760, Loss: 1.620246\n",
      "Iteration 2780, Loss: 1.626490\n",
      "Iteration 2800, Loss: 1.630780\n",
      "Iteration 2820, Loss: 1.631606\n",
      "Iteration 2840, Loss: 1.632698\n",
      "Iteration 2860, Loss: 1.628939\n",
      "Iteration 2880, Loss: 1.622980\n",
      "Iteration 2900, Loss: 1.624253\n",
      "Iteration 2920, Loss: 1.631385\n",
      "Iteration 2940, Loss: 1.634757\n",
      "Iteration 2960, Loss: 1.632463\n",
      "Iteration 2980, Loss: 1.629462\n",
      "Iteration 3000, Loss: 1.632278\n",
      "Iteration 3020, Loss: 1.639865\n",
      "Iteration 3040, Loss: 1.644358\n",
      "Iteration 3060, Loss: 1.640425\n",
      "Iteration 3080, Loss: 1.629084\n",
      "Iteration 3100, Loss: 1.620320\n",
      "Iteration 3120, Loss: 1.620245\n",
      "Iteration 3140, Loss: 1.621693\n",
      "Iteration 3160, Loss: 1.622945\n",
      "Iteration 3180, Loss: 1.624128\n",
      "Iteration 3200, Loss: 1.622299\n",
      "Iteration 3220, Loss: 1.618129\n",
      "Iteration 3240, Loss: 1.612027\n",
      "Iteration 3260, Loss: 1.606228\n",
      "Iteration 3280, Loss: 1.610779\n",
      "Iteration 3300, Loss: 1.616655\n",
      "Iteration 3320, Loss: 1.609092\n",
      "Iteration 3340, Loss: 1.610662\n",
      "Iteration 3360, Loss: 1.619190\n",
      "Iteration 3380, Loss: 1.618175\n",
      "Iteration 3400, Loss: 1.608960\n",
      "Iteration 3420, Loss: 1.603966\n",
      "Iteration 3440, Loss: 1.606289\n",
      "Iteration 3460, Loss: 1.603697\n",
      "Iteration 3480, Loss: 1.594637\n",
      "Iteration 3500, Loss: 1.590446\n",
      "Iteration 3520, Loss: 1.588120\n",
      "Iteration 3540, Loss: 1.582878\n",
      "Iteration 3560, Loss: 1.581982\n",
      "Iteration 3580, Loss: 1.582600\n",
      "Iteration 3600, Loss: 1.582846\n",
      "Iteration 3620, Loss: 1.587017\n",
      "Iteration 3640, Loss: 1.593033\n",
      "Iteration 3660, Loss: 1.596571\n",
      "Iteration 3680, Loss: 1.582214\n",
      "Iteration 3700, Loss: 1.569598\n",
      "Iteration 3720, Loss: 1.576494\n",
      "Iteration 3740, Loss: 1.582184\n",
      "Iteration 3760, Loss: 1.595510\n",
      "Iteration 3780, Loss: 1.592782\n",
      "Iteration 3800, Loss: 1.599434\n",
      "Iteration 3820, Loss: 1.610728\n",
      "Iteration 3840, Loss: 1.603396\n",
      "Iteration 3860, Loss: 1.604797\n",
      "Iteration 3880, Loss: 1.603132\n",
      "Iteration 3900, Loss: 1.605336\n",
      "Iteration 3920, Loss: 1.603928\n",
      "Iteration 3940, Loss: 1.595116\n",
      "Iteration 3960, Loss: 1.586016\n",
      "Iteration 3980, Loss: 1.587015\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "## Training\n",
    "iters = 4000  \n",
    "alpha = 0.01\n",
    "\n",
    "\n",
    "for c in range(iters):\n",
    "    # Clear all gradients\n",
    "    for p in updater_params:\n",
    "        if p.grad is not None:\n",
    "            p.grad.zero_()\n",
    "\n",
    "    ## Step 1: Get current predictor weights and decompose them\n",
    "    with torch.no_grad():  \n",
    "        current_W = predictor[0].W.detach().cpu().numpy()  # 784 x 10\n",
    "        LR_concat = decomposition(current_W, k=1)  # 794 elements\n",
    "    \n",
    "    ## Step 2: Forward pass through updater\n",
    "    for layer in updater:\n",
    "        layer.training = True\n",
    "    \n",
    "    # Convert decomposed weights to tensor and pass through updater\n",
    "    updater_input = torch.tensor(LR_concat, dtype=torch.float32, requires_grad=True).unsqueeze(0).to(device)\n",
    "    \n",
    "    updater_output = updater_input\n",
    "    for layer in updater:\n",
    "        updater_output = layer(updater_output)\n",
    "    \n",
    "    # Reconstruct weight update from updater output\n",
    "    L_update = updater_output[:, :784].reshape(784, 1)\n",
    "    R_update = updater_output[:, 784:].reshape(1, 10)\n",
    "    W_update = L_update @ R_update\n",
    "    \n",
    "    ## Step 3: Compute loss with updated weights\n",
    "    for layer in predictor:\n",
    "        layer.training = False\n",
    "    \n",
    "    # Apply weight update and compute forward pass\n",
    "    updated_weights = predictor[0].W + W_update * 0.1\n",
    "    predictions = X_train @ updated_weights \n",
    "    loss = f.cross_entropy(predictions, Y_train)\n",
    "    \n",
    "    ## Step 4: Backward pass and update\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update predictor weights (detached to avoid double backward)\n",
    "    with torch.no_grad():\n",
    "        predictor[0].W += W_update.detach() * 0.1\n",
    "    \n",
    "    # Update updater parameters\n",
    "    for p in updater_params:\n",
    "        if p.grad is not None:\n",
    "            p.data -= alpha * p.grad\n",
    "\n",
    "    if c % 20 == 0:\n",
    "        print(f\"Iteration {c:4d}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64137af1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64137af1",
    "outputId": "d6df42eb-a33a-4b60-fc05-c20ed6551407"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ## Training\n",
    "# iters = 4000\n",
    "# alpha = 0.01\n",
    "\n",
    "# for c in range(iters):\n",
    "#     ## Forward Pass through predictor\n",
    "#     for layer in predictor:\n",
    "#       layer.training = False\n",
    "#     x = X_train\n",
    "#     print(x.shape)\n",
    "#     for layer in predictor:\n",
    "#       x = layer(x)\n",
    "#     # Loss\n",
    "#     Loss = f.cross_entropy(x, Y_train)\n",
    "\n",
    "\n",
    "#     ## Forward Pass through updater\n",
    "#     for layer in layers:\n",
    "#       layer.training = True\n",
    "\n",
    "#     # Full SVD\n",
    "#     A = predictor[0].W.detach().cpu().numpy()  # 784 x 10\n",
    "#     LR_concat = decomposition(A, k=1) # 794\n",
    "#     print(\"LR_concat shape:\", LR_concat.shape)\n",
    "\n",
    "#     i = torch.stack([torch.tensor(LR_concat).to(device)])\n",
    "#     print(i.shape)\n",
    "\n",
    "#     for layer in layers:\n",
    "#       i = layer(i)\n",
    "#     print(i.shape)\n",
    "    \n",
    "#     L_update = i[:, :784].reshape(784, 1)\n",
    "#     R_update = i[:, 784:].reshape(1, 10)\n",
    "#     predictor_W_update = L_update @ R_update\n",
    "\n",
    "\n",
    "#     ## Weight update for predicter\n",
    "#     predictor[0].W+=predictor_W_update\n",
    "\n",
    "\n",
    "#     # Calculating Gradient for updater model\n",
    "#     for layer in layers:\n",
    "#       layer.result.retain_grad() # This stores grad of layers like Tanh that have no params to update\n",
    "\n",
    "#     for p in params:\n",
    "#         p.grad = None\n",
    "\n",
    "#     Loss.backward()\n",
    "\n",
    "#     # Weight Update for updater model\n",
    "#     for p in params:\n",
    "#         p.data += -alpha * p.grad\n",
    "\n",
    "\n",
    "\n",
    "#     if c % (iters/20) == 0:\n",
    "#         print(Loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadd2ac6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eadd2ac6",
    "outputId": "3c2a48ec-2b57-421b-f6b6-6bb794d0ba23"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (60000x784 and 794x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m       x \u001b[38;5;241m=\u001b[39m layer(x)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mcross_entropy(x, Y)\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43maccuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | test accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy(X_test,\u001b[38;5;250m \u001b[39mY_test)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss(X_train,\u001b[38;5;250m \u001b[39mY_train)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | test loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss(X_test,\u001b[38;5;250m \u001b[39mY_test)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[29], line 7\u001b[0m, in \u001b[0;36maccuracy\u001b[0;34m(X, Y)\u001b[0m\n\u001b[1;32m      5\u001b[0m x \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m layers:\n\u001b[0;32m----> 7\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m probs \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39msoftmax(x, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      9\u001b[0m answers \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[25], line 14\u001b[0m, in \u001b[0;36mLinear.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 14\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mequal(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mB, torch\u001b[38;5;241m.\u001b[39mtensor([])\u001b[38;5;241m.\u001b[39mto(device)): \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;129;43m@self\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mB\n\u001b[1;32m     15\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult \u001b[38;5;241m=\u001b[39m x\u001b[38;5;129m@self\u001b[39m\u001b[38;5;241m.\u001b[39mW\n\u001b[1;32m     16\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (60000x784 and 794x512)"
     ]
    }
   ],
   "source": [
    "def accuracy(X, Y):\n",
    "    for layer in layers:\n",
    "      layer.training=False\n",
    "    # Forward\n",
    "    x = X\n",
    "    for layer in layers:\n",
    "      x = layer(x)\n",
    "    probs = f.softmax(x, 1)\n",
    "    answers = x.argmax(1)\n",
    "    c = 0\n",
    "    for a, y in zip(answers, Y):\n",
    "        if a==y: c+=1\n",
    "    return c / answers.shape[0] * 100\n",
    "\n",
    "def loss(X, Y):\n",
    "    x = X\n",
    "    for layer in layers:\n",
    "      x = layer(x)\n",
    "    return f.cross_entropy(x, Y)\n",
    "\n",
    "\n",
    "print(f\"train accuracy: {accuracy(X_train, Y_train)} | test accuracy: {accuracy(X_test, Y_test)}\")\n",
    "print(f\"train loss: {loss(X_train, Y_train)} | test loss: {loss(X_test, Y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
