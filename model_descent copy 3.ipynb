{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "Rlc2iOtbNxMB",
   "metadata": {
    "id": "Rlc2iOtbNxMB"
   },
   "outputs": [],
   "source": [
    "#!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "329a74b6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "329a74b6",
    "outputId": "3ebe770e-e155-4d9a-a169-ef547a9e14de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "## Please install torch and datasets\n",
    "import torch\n",
    "from torchvision.transforms import functional as t\n",
    "import torch.nn.functional as f\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0934cf3a",
   "metadata": {
    "id": "0934cf3a"
   },
   "outputs": [],
   "source": [
    "## Loading our dataset\n",
    "ds = load_dataset(\"ylecun/mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb5cbf19",
   "metadata": {
    "id": "fb5cbf19"
   },
   "outputs": [],
   "source": [
    "## Data splits\n",
    "\n",
    "X_train_p = ds[\"train\"][\"image\"]\n",
    "Y_train = ds[\"train\"][\"label\"]\n",
    "X_test_p = ds[\"test\"][\"image\"]\n",
    "Y_test = ds[\"test\"][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d0c8b78",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3d0c8b78",
    "outputId": "fa7568ff-42b2-488a-c914-37d068da6707"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 1, 28, 28]) torch.Size([10000, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "## PIL to Tensors\n",
    "\n",
    "X_train = [t.pil_to_tensor(x) for x in X_train_p]\n",
    "X_test = [t.pil_to_tensor(x) for x in X_test_p]\n",
    "X_train = torch.stack(X_train).to(device)\n",
    "X_test = torch.stack(X_test).to(device)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d32ba99f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d32ba99f",
    "outputId": "d6b9fdd1-b141-4d0c-f63c-eb1833ecbaa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28]) torch.Size([10000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "## Fixing the shape\n",
    "\n",
    "X_train = X_train.view(-1, 28, 28)\n",
    "X_test = X_test.view(-1, 28, 28)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34acb755",
   "metadata": {
    "id": "34acb755"
   },
   "outputs": [],
   "source": [
    "## Making labels into tensors\n",
    "\n",
    "Y_train = torch.tensor(Y_train).to(device)\n",
    "Y_test = torch.tensor(Y_test).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5239aee9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5239aee9",
    "outputId": "a7b8d46a-c476-4704-b77e-c3344066ccca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 784]) torch.Size([10000, 784])\n"
     ]
    }
   ],
   "source": [
    "## Flattening the image as DNN takes flat tensor as input\n",
    "\n",
    "X_train = X_train.view(-1, 784).float() / 255.0\n",
    "X_test = X_test.view(-1, 784).float() / 255.0\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sGEKSOBrVZxy",
   "metadata": {
    "id": "sGEKSOBrVZxy"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import svd\n",
    "\n",
    "\n",
    "\n",
    "class Linear():\n",
    "  def __init__(self, input_dims, output_dims, B=True, last=False):\n",
    "    self.training = True\n",
    "    self.W = (torch.randn(input_dims, output_dims) * (5/3) / (input_dims**0.5)).to(device) if not last else (torch.randn(input_dims, output_dims) * (5/3) / (input_dims**0.5) * 0.1).to(device)\n",
    "    if B: self.B = torch.randn(output_dims).to(device) if not last else (torch.randn(output_dims) * 0.1).to(device)\n",
    "    else: self.B = torch.tensor([]).to(device)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    if not torch.equal(self.B, torch.tensor([]).to(device)): self.result = x@self.W + self.B\n",
    "    else: self.result = x@self.W\n",
    "    return self.result\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.W] + [self.B]\n",
    "\n",
    "\n",
    "class Tanh():\n",
    "  def __init__(self):\n",
    "    self.training = True\n",
    "    return None\n",
    "\n",
    "  def __call__(self, x):\n",
    "    self.result = torch.tanh(x)\n",
    "    return self.result\n",
    "\n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "class Dropout():\n",
    "  def __init__(self, batch_size, output_dims, rate=0.9):\n",
    "    self.training = True\n",
    "    self.rate = rate\n",
    "    self.factor = (torch.rand(batch_size, output_dims) < self.rate).int().to(device)\n",
    "    return None\n",
    "\n",
    "  def __call__(self, x):\n",
    "    if self.training: self.result = x * self.factor\n",
    "    else: self.result = x\n",
    "    return self.result\n",
    "\n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "\n",
    "def decomposition(A, k=1):\n",
    "\n",
    "    # SVD\n",
    "    U, S, VT = svd(A, full_matrices=False)\n",
    "\n",
    "\n",
    "    # Truncate to rank-k\n",
    "    U_k = U[:, :k]                # (784 x k)\n",
    "    S_k = np.diag(S[:k])          # (k x k)\n",
    "    VT_k = VT[:k, :]              # (k x 10)\n",
    "\n",
    "    # Factor A_k = L @ R, where L and R are low-rank factors\n",
    "    sqrt_S_k = np.sqrt(S_k)       # (k x k)\n",
    "    L = U_k @ sqrt_S_k            # (784 x k)\n",
    "    R = sqrt_S_k @ VT_k           # (k x 10)\n",
    "\n",
    "    L_flat = L.flatten()\n",
    "    R_flat = R.flatten()\n",
    "    LR_concat = np.concatenate([L_flat, R_flat])\n",
    "\n",
    "    return LR_concat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PK1PLMxCWaM9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PK1PLMxCWaM9",
    "outputId": "b2943435-c8dd-4304-a8d2-8d1c5dece336"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1077274\n"
     ]
    }
   ],
   "source": [
    "n1 = 512\n",
    "n2 = 256\n",
    "n3 = 512\n",
    "n4 = 794\n",
    "batch_size = 1\n",
    "\n",
    "updater = [\n",
    "    Linear(794, n1), Tanh(), Dropout(batch_size, n1),\n",
    "    Linear(n1, n2), Tanh(), Dropout(batch_size, n2),\n",
    "    Linear(n2, n3), Tanh(), Dropout(batch_size, n3),\n",
    "   Linear(n3, n4, last=True),\n",
    "    \n",
    "]\n",
    "\n",
    "predictor = [\n",
    "    Linear(784, 10, last=True, B=False)\n",
    "]\n",
    "\n",
    "updater_params = [p for layer in updater for p in layer.parameters()]\n",
    "numparams = 0\n",
    "for p in updater_params:\n",
    "    p.requires_grad = True\n",
    "    numparams += p.numel()\n",
    "\n",
    "\n",
    "predictor_params = [p for layer in updater for p in layer.parameters()]\n",
    "numparams = 0\n",
    "for p in updater_params:\n",
    "    p.requires_grad = True\n",
    "    numparams += p.numel()\n",
    "print(numparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e298304",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     13\u001b[0m     current_W \u001b[38;5;241m=\u001b[39m predictor[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mW\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mpredictor\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n\u001b[1;32m     15\u001b[0m     LR_concat \u001b[38;5;241m=\u001b[39m decomposition(current_W, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Forward pass through updater\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "## Training loop for updater and predictor\n",
    "iters = 4000  \n",
    "alpha = 0.01\n",
    "\n",
    "for c in range(iters):\n",
    "    # Zero updater gradients\n",
    "    for p in updater_params:\n",
    "        if p.grad is not None:\n",
    "            p.grad.zero_()\n",
    "\n",
    "    # Decompose predictor weights\n",
    "    with torch.no_grad():\n",
    "        current_W = predictor[0].W.detach().cpu().numpy()\n",
    "        print(predictor[0].W.grad.shape)\n",
    "        LR_concat = decomposition(current_W, k=1)\n",
    "    \n",
    "    # Forward pass through updater\n",
    "    for layer in updater:\n",
    "        layer.training = True\n",
    "    updater_input = torch.tensor(LR_concat, dtype=torch.float32, requires_grad=True).unsqueeze(0).to(device)\n",
    "    updater_output = updater_input\n",
    "    for layer in updater:\n",
    "        updater_output = layer(updater_output)\n",
    "    L_update = updater_output[:, :784].reshape(784, 1)\n",
    "    R_update = updater_output[:, 784:].reshape(1, 10)\n",
    "    W_update = L_update @ R_update\n",
    "\n",
    "    # Compute loss with updated weights\n",
    "    for layer in predictor:\n",
    "        layer.training = False\n",
    "    updated_weights = predictor[0].W + W_update * 0.1\n",
    "    predictions = X_train @ updated_weights \n",
    "    loss = f.cross_entropy(predictions, Y_train)\n",
    "    \n",
    "    # Backward and update\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        predictor[0].W += W_update.detach() * 0.1\n",
    "    for p in updater_params:\n",
    "        if p.grad is not None:\n",
    "            p.data -= alpha * p.grad\n",
    "\n",
    "    if c % 20 == 0:\n",
    "        print(f\"Iteration {c:4d}, Loss: {loss.item():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64137af1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64137af1",
    "outputId": "d6df42eb-a33a-4b60-fc05-c20ed6551407"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ## Training\n",
    "# iters = 4000\n",
    "# alpha = 0.01\n",
    "\n",
    "# for c in range(iters):\n",
    "#     ## Forward Pass through predictor\n",
    "#     for layer in predictor:\n",
    "#       layer.training = False\n",
    "#     x = X_train\n",
    "#     print(x.shape)\n",
    "#     for layer in predictor:\n",
    "#       x = layer(x)\n",
    "#     # Loss\n",
    "#     Loss = f.cross_entropy(x, Y_train)\n",
    "\n",
    "\n",
    "#     ## Forward Pass through updater\n",
    "#     for layer in layers:\n",
    "#       layer.training = True\n",
    "\n",
    "#     # Full SVD\n",
    "#     A = predictor[0].W.detach().cpu().numpy()  # 784 x 10\n",
    "#     LR_concat = decomposition(A, k=1) # 794\n",
    "#     print(\"LR_concat shape:\", LR_concat.shape)\n",
    "\n",
    "#     i = torch.stack([torch.tensor(LR_concat).to(device)])\n",
    "#     print(i.shape)\n",
    "\n",
    "#     for layer in layers:\n",
    "#       i = layer(i)\n",
    "#     print(i.shape)\n",
    "    \n",
    "#     L_update = i[:, :784].reshape(784, 1)\n",
    "#     R_update = i[:, 784:].reshape(1, 10)\n",
    "#     predictor_W_update = L_update @ R_update\n",
    "\n",
    "\n",
    "#     ## Weight update for predicter\n",
    "#     predictor[0].W+=predictor_W_update\n",
    "\n",
    "\n",
    "#     # Calculating Gradient for updater model\n",
    "#     for layer in layers:\n",
    "#       layer.result.retain_grad() # This stores grad of layers like Tanh that have no params to update\n",
    "\n",
    "#     for p in params:\n",
    "#         p.grad = None\n",
    "\n",
    "#     Loss.backward()\n",
    "\n",
    "#     # Weight Update for updater model\n",
    "#     for p in params:\n",
    "#         p.data += -alpha * p.grad\n",
    "\n",
    "\n",
    "\n",
    "#     if c % (iters/20) == 0:\n",
    "#         print(Loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eadd2ac6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eadd2ac6",
    "outputId": "3c2a48ec-2b57-421b-f6b6-6bb794d0ba23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 76.36833333333334 | test accuracy: 77.13\n",
      "train loss: 1.5593186616897583 | test loss: 1.4697450399398804\n"
     ]
    }
   ],
   "source": [
    "def accuracy(X, Y, layersv=predictor):\n",
    "    for layer in layersv:\n",
    "      layer.training=False\n",
    "    # Forward\n",
    "    x = X\n",
    "    for layer in layersv:\n",
    "      x = layer(x)\n",
    "    probs = f.softmax(x, 1)\n",
    "    answers = x.argmax(1)\n",
    "    c = 0\n",
    "    for a, y in zip(answers, Y):\n",
    "        if a==y: c+=1\n",
    "    return c / answers.shape[0] * 100\n",
    "\n",
    "def loss(X, Y, layersv = predictor):\n",
    "    x = X\n",
    "    for layer in layersv:\n",
    "      x = layer(x)\n",
    "    return f.cross_entropy(x, Y)\n",
    "\n",
    "print(f\"train accuracy: {accuracy(X_train, Y_train)} | test accuracy: {accuracy(X_test, Y_test)}\")\n",
    "print(f\"train loss: {loss(X_train, Y_train)} | test loss: {loss(X_test, Y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
