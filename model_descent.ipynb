{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "Rlc2iOtbNxMB",
   "metadata": {
    "id": "Rlc2iOtbNxMB"
   },
   "outputs": [],
   "source": [
    "#!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "329a74b6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "329a74b6",
    "outputId": "3ebe770e-e155-4d9a-a169-ef547a9e14de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "## Please install torch and datasets\n",
    "import torch\n",
    "from torchvision.transforms import functional as t\n",
    "import torch.nn.functional as f\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0934cf3a",
   "metadata": {
    "id": "0934cf3a"
   },
   "outputs": [],
   "source": [
    "## Loading our dataset\n",
    "ds = load_dataset(\"ylecun/mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb5cbf19",
   "metadata": {
    "id": "fb5cbf19"
   },
   "outputs": [],
   "source": [
    "## Data splits\n",
    "\n",
    "X_train_p = ds[\"train\"][\"image\"]\n",
    "Y_train = ds[\"train\"][\"label\"]\n",
    "X_test_p = ds[\"test\"][\"image\"]\n",
    "Y_test = ds[\"test\"][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d0c8b78",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3d0c8b78",
    "outputId": "fa7568ff-42b2-488a-c914-37d068da6707"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 1, 28, 28]) torch.Size([10000, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "## PIL to Tensors\n",
    "\n",
    "X_train = [t.pil_to_tensor(x) for x in X_train_p]\n",
    "X_test = [t.pil_to_tensor(x) for x in X_test_p]\n",
    "X_train = torch.stack(X_train).to(device)\n",
    "X_test = torch.stack(X_test).to(device)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d32ba99f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d32ba99f",
    "outputId": "d6b9fdd1-b141-4d0c-f63c-eb1833ecbaa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28]) torch.Size([10000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "## Fixing the shape\n",
    "\n",
    "X_train = X_train.view(-1, 28, 28)\n",
    "X_test = X_test.view(-1, 28, 28)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34acb755",
   "metadata": {
    "id": "34acb755"
   },
   "outputs": [],
   "source": [
    "## Making labels into tensors\n",
    "\n",
    "Y_train = torch.tensor(Y_train).to(device)\n",
    "Y_test = torch.tensor(Y_test).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5239aee9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5239aee9",
    "outputId": "a7b8d46a-c476-4704-b77e-c3344066ccca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 784]) torch.Size([10000, 784])\n"
     ]
    }
   ],
   "source": [
    "## Flattening the image as DNN takes flat tensor as input\n",
    "\n",
    "X_train = X_train.view(-1, 784).float() / 255.0\n",
    "X_test = X_test.view(-1, 784).float() / 255.0\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sGEKSOBrVZxy",
   "metadata": {
    "id": "sGEKSOBrVZxy"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import svd\n",
    "\n",
    "\n",
    "\n",
    "class Linear():\n",
    "  def __init__(self, input_dims, output_dims, B=True, last=False):\n",
    "    self.training = True\n",
    "    self.W = (torch.randn(input_dims, output_dims) * (5/3) / (input_dims**0.5)).to(device) if not last else (torch.randn(input_dims, output_dims) * (5/3) / (input_dims**0.5) * 0.1).to(device)\n",
    "    if B: self.B = torch.randn(output_dims).to(device) if not last else (torch.randn(output_dims) * 0.1).to(device)\n",
    "    else: self.B = torch.tensor([]).to(device)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    if not torch.equal(self.B, torch.tensor([]).to(device)): self.result = x@self.W + self.B\n",
    "    else: self.result = x@self.W\n",
    "    return self.result\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.W] + [self.B]\n",
    "\n",
    "\n",
    "class Tanh():\n",
    "  def __init__(self):\n",
    "    self.training = True\n",
    "    return None\n",
    "\n",
    "  def __call__(self, x):\n",
    "    self.result = torch.tanh(x)\n",
    "    return self.result\n",
    "\n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "class Dropout():\n",
    "  def __init__(self, batch_size, output_dims, rate=0.9):\n",
    "    self.training = True\n",
    "    self.rate = rate\n",
    "    self.factor = (torch.rand(batch_size, output_dims) < self.rate).int().to(device)\n",
    "    return None\n",
    "\n",
    "  def __call__(self, x):\n",
    "    if self.training: self.result = x * self.factor\n",
    "    else: self.result = x\n",
    "    return self.result\n",
    "\n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "\n",
    "def decomposition(A, k=1):\n",
    "\n",
    "    # SVD\n",
    "    U, S, VT = svd(A, full_matrices=False)\n",
    "\n",
    "\n",
    "    # Truncate to rank-k\n",
    "    U_k = U[:, :k]                # (784 x k)\n",
    "    S_k = np.diag(S[:k])          # (k x k)\n",
    "    VT_k = VT[:k, :]              # (k x 10)\n",
    "\n",
    "    # Factor A_k = L @ R, where L and R are low-rank factors\n",
    "    sqrt_S_k = np.sqrt(S_k)       # (k x k)\n",
    "    L = U_k @ sqrt_S_k            # (784 x k)\n",
    "    R = sqrt_S_k @ VT_k           # (k x 10)\n",
    "\n",
    "    L_flat = L.flatten()\n",
    "    R_flat = R.flatten()\n",
    "    LR_concat = np.concatenate([L_flat, R_flat])\n",
    "\n",
    "    return LR_concat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "PK1PLMxCWaM9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PK1PLMxCWaM9",
    "outputId": "b2943435-c8dd-4304-a8d2-8d1c5dece336"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "814362\n"
     ]
    }
   ],
   "source": [
    "n1 = 512\n",
    "n2 = 256\n",
    "n3 = 512\n",
    "n4 = 794\n",
    "batch_size = 1\n",
    "\n",
    "updater = [\n",
    "    Linear(794, n1), Tanh(), Dropout(batch_size, n1),\n",
    "    #Linear(n1, n2), Tanh(), Dropout(batch_size, n2),\n",
    "    #Linear(n2, n3), Tanh(), Dropout(batch_size, n3),\n",
    "   Linear(n1, 794, last=True),\n",
    "    \n",
    "]\n",
    "\n",
    "predictor = [\n",
    "    Linear(784, 10, last=True, B=False)\n",
    "]\n",
    "\n",
    "updater_params = [p for layer in updater for p in layer.parameters()]\n",
    "numparams = 0\n",
    "for p in updater_params:\n",
    "    p.requires_grad = True\n",
    "    numparams += p.numel()\n",
    "\n",
    "\n",
    "predictor_params = [p for layer in predictor for p in layer.parameters()]\n",
    "for p in predictor_params:\n",
    "    p.requires_grad = True\n",
    "print(numparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e298304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0, Loss: 1.831913\n",
      "Iteration   20, Loss: 1.894346\n",
      "Iteration   40, Loss: 1.910147\n",
      "Iteration   60, Loss: 1.885522\n",
      "Iteration   80, Loss: 1.856037\n",
      "Iteration  100, Loss: 1.845506\n",
      "Iteration  120, Loss: 1.807259\n",
      "Iteration  140, Loss: 1.823778\n",
      "Iteration  160, Loss: 1.835210\n",
      "Iteration  180, Loss: 1.852521\n",
      "Iteration  200, Loss: 1.868489\n",
      "Iteration  220, Loss: 1.873372\n",
      "Iteration  240, Loss: 1.870271\n",
      "Iteration  260, Loss: 1.854848\n",
      "Iteration  280, Loss: 1.861901\n",
      "Iteration  300, Loss: 1.869761\n",
      "Iteration  320, Loss: 1.861971\n",
      "Iteration  340, Loss: 1.847721\n",
      "Iteration  360, Loss: 1.811841\n",
      "Iteration  380, Loss: 1.815572\n",
      "Iteration  400, Loss: 1.832138\n",
      "Iteration  420, Loss: 1.857014\n",
      "Iteration  440, Loss: 1.853353\n",
      "Iteration  460, Loss: 1.811177\n",
      "Iteration  480, Loss: 1.837409\n",
      "Iteration  500, Loss: 1.866915\n",
      "Iteration  520, Loss: 1.869753\n",
      "Iteration  540, Loss: 1.877122\n",
      "Iteration  560, Loss: 1.886601\n",
      "Iteration  580, Loss: 1.889726\n",
      "Iteration  600, Loss: 1.883915\n",
      "Iteration  620, Loss: 1.868430\n",
      "Iteration  640, Loss: 1.827688\n",
      "Iteration  660, Loss: 1.812735\n",
      "Iteration  680, Loss: 1.849714\n",
      "Iteration  700, Loss: 1.851992\n",
      "Iteration  720, Loss: 1.833178\n",
      "Iteration  740, Loss: 1.836819\n",
      "Iteration  760, Loss: 1.824904\n",
      "Iteration  780, Loss: 1.815363\n",
      "Iteration  800, Loss: 1.817530\n",
      "Iteration  820, Loss: 1.837392\n",
      "Iteration  840, Loss: 1.851435\n",
      "Iteration  860, Loss: 1.861140\n",
      "Iteration  880, Loss: 1.865243\n",
      "Iteration  900, Loss: 1.860521\n",
      "Iteration  920, Loss: 1.860588\n",
      "Iteration  940, Loss: 1.861408\n",
      "Iteration  960, Loss: 1.854755\n",
      "Iteration  980, Loss: 1.843069\n",
      "Iteration 1000, Loss: 1.842603\n",
      "Iteration 1020, Loss: 1.839687\n",
      "Iteration 1040, Loss: 1.832727\n",
      "Iteration 1060, Loss: 1.829473\n",
      "Iteration 1080, Loss: 1.831151\n",
      "Iteration 1100, Loss: 1.834632\n",
      "Iteration 1120, Loss: 1.832275\n",
      "Iteration 1140, Loss: 1.820209\n",
      "Iteration 1160, Loss: 1.809244\n",
      "Iteration 1180, Loss: 1.804783\n",
      "Iteration 1200, Loss: 1.798770\n",
      "Iteration 1220, Loss: 1.817206\n",
      "Iteration 1240, Loss: 1.842649\n",
      "Iteration 1260, Loss: 1.862032\n",
      "Iteration 1280, Loss: 1.869742\n",
      "Iteration 1300, Loss: 1.860530\n",
      "Iteration 1320, Loss: 1.840605\n",
      "Iteration 1340, Loss: 1.805239\n",
      "Iteration 1360, Loss: 1.807459\n",
      "Iteration 1380, Loss: 1.810516\n",
      "Iteration 1400, Loss: 1.799437\n",
      "Iteration 1420, Loss: 1.819048\n",
      "Iteration 1440, Loss: 1.834694\n",
      "Iteration 1460, Loss: 1.841717\n",
      "Iteration 1480, Loss: 1.850713\n",
      "Iteration 1500, Loss: 1.861953\n",
      "Iteration 1520, Loss: 1.870648\n",
      "Iteration 1540, Loss: 1.869960\n",
      "Iteration 1560, Loss: 1.857907\n",
      "Iteration 1580, Loss: 1.836455\n",
      "Iteration 1600, Loss: 1.821426\n",
      "Iteration 1620, Loss: 1.820530\n",
      "Iteration 1640, Loss: 1.822644\n",
      "Iteration 1660, Loss: 1.821570\n",
      "Iteration 1680, Loss: 1.814552\n",
      "Iteration 1700, Loss: 1.827535\n",
      "Iteration 1720, Loss: 1.839328\n",
      "Iteration 1740, Loss: 1.840462\n",
      "Iteration 1760, Loss: 1.841044\n",
      "Iteration 1780, Loss: 1.837545\n",
      "Iteration 1800, Loss: 1.835796\n",
      "Iteration 1820, Loss: 1.840096\n",
      "Iteration 1840, Loss: 1.843134\n",
      "Iteration 1860, Loss: 1.845010\n",
      "Iteration 1880, Loss: 1.846107\n",
      "Iteration 1900, Loss: 1.843924\n",
      "Iteration 1920, Loss: 1.837133\n",
      "Iteration 1940, Loss: 1.828853\n",
      "Iteration 1960, Loss: 1.823685\n",
      "Iteration 1980, Loss: 1.825295\n",
      "Iteration 2000, Loss: 1.833286\n",
      "Iteration 2020, Loss: 1.840648\n",
      "Iteration 2040, Loss: 1.841193\n",
      "Iteration 2060, Loss: 1.836298\n",
      "Iteration 2080, Loss: 1.827845\n",
      "Iteration 2100, Loss: 1.813792\n",
      "Iteration 2120, Loss: 1.814491\n",
      "Iteration 2140, Loss: 1.826684\n",
      "Iteration 2160, Loss: 1.817043\n",
      "Iteration 2180, Loss: 1.808048\n",
      "Iteration 2200, Loss: 1.833435\n",
      "Iteration 2220, Loss: 1.843652\n",
      "Iteration 2240, Loss: 1.847240\n",
      "Iteration 2260, Loss: 1.846707\n",
      "Iteration 2280, Loss: 1.841060\n",
      "Iteration 2300, Loss: 1.834631\n",
      "Iteration 2320, Loss: 1.835433\n",
      "Iteration 2340, Loss: 1.842101\n",
      "Iteration 2360, Loss: 1.848136\n",
      "Iteration 2380, Loss: 1.852300\n",
      "Iteration 2400, Loss: 1.853170\n",
      "Iteration 2420, Loss: 1.848055\n",
      "Iteration 2440, Loss: 1.841215\n",
      "Iteration 2460, Loss: 1.855200\n",
      "Iteration 2480, Loss: 1.866000\n",
      "Iteration 2500, Loss: 1.857782\n",
      "Iteration 2520, Loss: 1.838836\n",
      "Iteration 2540, Loss: 1.830256\n",
      "Iteration 2560, Loss: 1.828779\n",
      "Iteration 2580, Loss: 1.828524\n",
      "Iteration 2600, Loss: 1.827770\n",
      "Iteration 2620, Loss: 1.821386\n",
      "Iteration 2640, Loss: 1.809971\n",
      "Iteration 2660, Loss: 1.809018\n",
      "Iteration 2680, Loss: 1.833126\n",
      "Iteration 2700, Loss: 1.858653\n",
      "Iteration 2720, Loss: 1.869559\n",
      "Iteration 2740, Loss: 1.863959\n",
      "Iteration 2760, Loss: 1.843741\n",
      "Iteration 2780, Loss: 1.819287\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m loss \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mcross_entropy(predictions, Y_train)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Backward and update\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     42\u001b[0m     predictor[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mW \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m W_update\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Training loop for updater and predictor\n",
    "iters = 4000  \n",
    "alpha = 0.01\n",
    "\n",
    "for c in range(iters):\n",
    "    # Zero updater gradients\n",
    "    for p in updater_params:\n",
    "        if p.grad is not None:\n",
    "            p.grad.zero_()\n",
    "\n",
    "    # Decompose predictor weights\n",
    "    with torch.no_grad():\n",
    "        #current_W = predictor[0].W.detach().cpu().numpy()\n",
    "        if predictor[0].W.grad != None: \n",
    "            current_W = predictor[0].W.grad.cpu().numpy()\n",
    "        else:\n",
    "            print(\"None update\")\n",
    "            current_W = torch.zeros((predictor[0].W.shape)).cpu().numpy()\n",
    "        LR_concat = decomposition(current_W, k=1)\n",
    "    \n",
    "    # Forward pass through updater\n",
    "    for layer in updater:\n",
    "        layer.training = True\n",
    "    updater_input = torch.tensor(LR_concat, dtype=torch.float32, requires_grad=True).unsqueeze(0).to(device)\n",
    "    updater_output = updater_input\n",
    "    for layer in updater:\n",
    "        updater_output = layer(updater_output)\n",
    "    L_update = updater_output[:, :784].reshape(784, 1)\n",
    "    R_update = updater_output[:, 784:].reshape(1, 10)\n",
    "    W_update = L_update @ R_update\n",
    "\n",
    "    # Compute loss with updated weights\n",
    "    for layer in predictor:\n",
    "        layer.training = False\n",
    "    updated_weights = predictor[0].W + W_update * 0.1\n",
    "    predictions = X_train @ updated_weights \n",
    "    loss = f.cross_entropy(predictions, Y_train)\n",
    "    \n",
    "    # Backward and update\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        predictor[0].W += W_update.detach() * 0.1\n",
    "    for p in updater_params:\n",
    "        if p.grad is not None:\n",
    "            p.data -= alpha * p.grad\n",
    "\n",
    "    if c % 20 == 0:\n",
    "        print(f\"Iteration {c:4d}, Loss: {loss.item():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64137af1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64137af1",
    "outputId": "d6df42eb-a33a-4b60-fc05-c20ed6551407"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ## Training\n",
    "# iters = 4000\n",
    "# alpha = 0.01\n",
    "\n",
    "# for c in range(iters):\n",
    "#     ## Forward Pass through predictor\n",
    "#     for layer in predictor:\n",
    "#       layer.training = False\n",
    "#     x = X_train\n",
    "#     print(x.shape)\n",
    "#     for layer in predictor:\n",
    "#       x = layer(x)\n",
    "#     # Loss\n",
    "#     Loss = f.cross_entropy(x, Y_train)\n",
    "\n",
    "\n",
    "#     ## Forward Pass through updater\n",
    "#     for layer in layers:\n",
    "#       layer.training = True\n",
    "\n",
    "#     # Full SVD\n",
    "#     A = predictor[0].W.detach().cpu().numpy()  # 784 x 10\n",
    "#     LR_concat = decomposition(A, k=1) # 794\n",
    "#     print(\"LR_concat shape:\", LR_concat.shape)\n",
    "\n",
    "#     i = torch.stack([torch.tensor(LR_concat).to(device)])\n",
    "#     print(i.shape)\n",
    "\n",
    "#     for layer in layers:\n",
    "#       i = layer(i)\n",
    "#     print(i.shape)\n",
    "    \n",
    "#     L_update = i[:, :784].reshape(784, 1)\n",
    "#     R_update = i[:, 784:].reshape(1, 10)\n",
    "#     predictor_W_update = L_update @ R_update\n",
    "\n",
    "\n",
    "#     ## Weight update for predicter\n",
    "#     predictor[0].W+=predictor_W_update\n",
    "\n",
    "\n",
    "#     # Calculating Gradient for updater model\n",
    "#     for layer in layers:\n",
    "#       layer.result.retain_grad() # This stores grad of layers like Tanh that have no params to update\n",
    "\n",
    "#     for p in params:\n",
    "#         p.grad = None\n",
    "\n",
    "#     Loss.backward()\n",
    "\n",
    "#     # Weight Update for updater model\n",
    "#     for p in params:\n",
    "#         p.data += -alpha * p.grad\n",
    "\n",
    "\n",
    "\n",
    "#     if c % (iters/20) == 0:\n",
    "#         print(Loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadd2ac6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eadd2ac6",
    "outputId": "3c2a48ec-2b57-421b-f6b6-6bb794d0ba23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 76.36833333333334 | test accuracy: 77.13\n",
      "train loss: 1.5593186616897583 | test loss: 1.4697450399398804\n"
     ]
    }
   ],
   "source": [
    "def accuracy(X, Y, layersv=predictor):\n",
    "    for layer in layersv:\n",
    "      layer.training=False\n",
    "    # Forward\n",
    "    x = X\n",
    "    for layer in layersv:\n",
    "      x = layer(x)\n",
    "    probs = f.softmax(x, 1)\n",
    "    answers = x.argmax(1)\n",
    "    c = 0\n",
    "    for a, y in zip(answers, Y):\n",
    "        if a==y: c+=1\n",
    "    return c / answers.shape[0] * 100\n",
    "\n",
    "def loss(X, Y, layersv = predictor):\n",
    "    x = X\n",
    "    for layer in layersv:\n",
    "      x = layer(x)\n",
    "    return f.cross_entropy(x, Y)\n",
    "\n",
    "print(f\"train accuracy: {accuracy(X_train, Y_train)} | test accuracy: {accuracy(X_test, Y_test)}\")\n",
    "print(f\"train loss: {loss(X_train, Y_train)} | test loss: {loss(X_test, Y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
