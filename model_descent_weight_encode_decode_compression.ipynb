{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "Rlc2iOtbNxMB",
   "metadata": {
    "id": "Rlc2iOtbNxMB"
   },
   "outputs": [],
   "source": [
    "#!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "329a74b6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "329a74b6",
    "outputId": "3ebe770e-e155-4d9a-a169-ef547a9e14de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "## Please install torch and datasets\n",
    "import torch\n",
    "from torchvision.transforms import functional as t\n",
    "import torch.nn.functional as f\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0934cf3a",
   "metadata": {
    "id": "0934cf3a"
   },
   "outputs": [],
   "source": [
    "## Loading our dataset\n",
    "ds = load_dataset(\"ylecun/mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb5cbf19",
   "metadata": {
    "id": "fb5cbf19"
   },
   "outputs": [],
   "source": [
    "## Data splits\n",
    "\n",
    "X_train_p = ds[\"train\"][\"image\"]\n",
    "Y_train = ds[\"train\"][\"label\"]\n",
    "X_test_p = ds[\"test\"][\"image\"]\n",
    "Y_test = ds[\"test\"][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d0c8b78",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3d0c8b78",
    "outputId": "fa7568ff-42b2-488a-c914-37d068da6707"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 1, 28, 28]) torch.Size([10000, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "## PIL to Tensors\n",
    "\n",
    "X_train = [t.pil_to_tensor(x) for x in X_train_p]\n",
    "X_test = [t.pil_to_tensor(x) for x in X_test_p]\n",
    "X_train = torch.stack(X_train).to(device)\n",
    "X_test = torch.stack(X_test).to(device)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d32ba99f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d32ba99f",
    "outputId": "d6b9fdd1-b141-4d0c-f63c-eb1833ecbaa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28]) torch.Size([10000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "## Fixing the shape\n",
    "\n",
    "X_train = X_train.view(-1, 28, 28)\n",
    "X_test = X_test.view(-1, 28, 28)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34acb755",
   "metadata": {
    "id": "34acb755"
   },
   "outputs": [],
   "source": [
    "## Making labels into tensors\n",
    "\n",
    "Y_train = torch.tensor(Y_train).to(device)\n",
    "Y_test = torch.tensor(Y_test).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5239aee9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5239aee9",
    "outputId": "a7b8d46a-c476-4704-b77e-c3344066ccca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 784]) torch.Size([10000, 784])\n"
     ]
    }
   ],
   "source": [
    "## Flattening the image as DNN takes flat tensor as input\n",
    "\n",
    "X_train = X_train.view(-1, 784).float() / 255.0\n",
    "X_test = X_test.view(-1, 784).float() / 255.0\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sGEKSOBrVZxy",
   "metadata": {
    "id": "sGEKSOBrVZxy"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import svd\n",
    "\n",
    "\n",
    "\n",
    "class Linear():\n",
    "  def __init__(self, input_dims, output_dims, B=True, last=False):\n",
    "    self.training = True\n",
    "    self.W = (torch.randn(input_dims, output_dims) * (5/3) / (input_dims**0.5)).to(device) if not last else (torch.randn(input_dims, output_dims) * (5/3) / (input_dims**0.5) * 0.1).to(device)\n",
    "    if B: self.B = torch.randn(output_dims).to(device) if not last else (torch.randn(output_dims) * 0.1).to(device)\n",
    "    else: self.B = torch.tensor([]).to(device)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    if not torch.equal(self.B, torch.tensor([]).to(device)): self.result = x@self.W + self.B\n",
    "    else: self.result = x@self.W\n",
    "    return self.result\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.W] + [self.B]\n",
    "\n",
    "\n",
    "class Tanh():\n",
    "  def __init__(self):\n",
    "    self.training = True\n",
    "    return None\n",
    "\n",
    "  def __call__(self, x):\n",
    "    self.result = torch.tanh(x)\n",
    "    return self.result\n",
    "\n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "class Dropout():\n",
    "  def __init__(self, batch_size, output_dims, rate=0.9):\n",
    "    self.training = True\n",
    "    self.rate = rate\n",
    "    self.factor = (torch.rand(batch_size, output_dims) < self.rate).int().to(device)\n",
    "    return None\n",
    "\n",
    "  def __call__(self, x):\n",
    "    if self.training: self.result = x * self.factor\n",
    "    else: self.result = x\n",
    "    return self.result\n",
    "\n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "\n",
    "def decomposition(A, k=1):\n",
    "\n",
    "    # SVD\n",
    "    U, S, VT = svd(A, full_matrices=False)\n",
    "\n",
    "\n",
    "    # Truncate to rank-k\n",
    "    U_k = U[:, :k]                # (784 x k)\n",
    "    S_k = np.diag(S[:k])          # (k x k)\n",
    "    VT_k = VT[:k, :]              # (k x 10)\n",
    "\n",
    "    # Factor A_k = L @ R, where L and R are low-rank factors\n",
    "    sqrt_S_k = np.sqrt(S_k)       # (k x k)\n",
    "    L = U_k @ sqrt_S_k            # (784 x k)\n",
    "    R = sqrt_S_k @ VT_k           # (k x 10)\n",
    "\n",
    "    L_flat = L.flatten()\n",
    "    R_flat = R.flatten()\n",
    "    LR_concat = np.concatenate([L_flat, R_flat])\n",
    "\n",
    "    return LR_concat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "PK1PLMxCWaM9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PK1PLMxCWaM9",
    "outputId": "b2943435-c8dd-4304-a8d2-8d1c5dece336"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updater parameters: 245862400, Predictor parameters: 7840\n"
     ]
    }
   ],
   "source": [
    "batch_size = 60000\n",
    "\n",
    "mid_dims = 7840*2\n",
    "\n",
    "W_u1 = (torch.randn(7840, mid_dims) * (5/3) / (7840**0.5)).to(device)\n",
    "W_u2 = (torch.randn(mid_dims, 7840) * (5/3) / (mid_dims**0.5)).to(device)\n",
    "W_p_t_1 = (torch.randn(784, 10) * (5/3) / (784**0.5)).to(device)\n",
    "\n",
    "W_u1.requires_grad = True\n",
    "W_u2.requires_grad = True\n",
    "W_p_t_1.requires_grad = False\n",
    "\n",
    "updaterparams = [W_u1, W_u2]\n",
    "\n",
    "print(f\"Updater parameters: {W_u1.numel() + W_u2.numel()}, Predictor parameters: {W_p_t_1.numel()}\")\n",
    "\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e298304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0, Loss: 2.509966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_596279/215743696.py:12: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  if W_p_t_1.grad is not None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration   80, Loss: 2.850971\n",
      "Iteration  160, Loss: 2.404255\n",
      "Iteration  240, Loss: 1.425523\n",
      "Iteration  320, Loss: 1.126065\n",
      "Iteration  400, Loss: 1.064981\n",
      "Iteration  480, Loss: 1.049344\n",
      "Iteration  560, Loss: 1.025473\n",
      "Iteration  640, Loss: 1.022153\n",
      "Iteration  720, Loss: 1.015481\n",
      "Iteration  800, Loss: 1.007365\n",
      "Iteration  880, Loss: 1.007718\n",
      "Iteration  960, Loss: 1.004401\n",
      "Iteration 1040, Loss: 1.005204\n",
      "Iteration 1120, Loss: 1.002558\n",
      "Iteration 1200, Loss: 0.999413\n",
      "Iteration 1280, Loss: 0.995939\n",
      "Iteration 1360, Loss: 0.995918\n",
      "Iteration 1440, Loss: 0.993679\n",
      "Iteration 1520, Loss: 0.990277\n",
      "Iteration 1600, Loss: 0.992854\n",
      "Iteration 1680, Loss: 0.991406\n",
      "Iteration 1760, Loss: 0.992883\n",
      "Iteration 1840, Loss: 0.986408\n",
      "Iteration 1920, Loss: 0.988986\n",
      "Iteration 2000, Loss: 0.986406\n",
      "Iteration 2080, Loss: 0.986427\n",
      "Iteration 2160, Loss: 0.985193\n",
      "Iteration 2240, Loss: 0.986785\n",
      "Iteration 2320, Loss: 0.982629\n",
      "Iteration 2400, Loss: 0.982730\n",
      "Iteration 2480, Loss: 0.986552\n",
      "Iteration 2560, Loss: 0.983537\n",
      "Iteration 2640, Loss: 0.981602\n",
      "Iteration 2720, Loss: 0.981703\n",
      "Iteration 2800, Loss: 0.982167\n",
      "Iteration 2880, Loss: 0.979837\n",
      "Iteration 2960, Loss: 0.976625\n",
      "Iteration 3040, Loss: 0.977232\n",
      "Iteration 3120, Loss: 0.978319\n",
      "Iteration 3200, Loss: 0.979166\n",
      "Iteration 3280, Loss: 0.980576\n",
      "Iteration 3360, Loss: 0.976909\n",
      "Iteration 3440, Loss: 0.975949\n",
      "Iteration 3520, Loss: 0.976817\n",
      "Iteration 3600, Loss: 0.975201\n",
      "Iteration 3680, Loss: 0.974761\n",
      "Iteration 3760, Loss: 0.980717\n",
      "Iteration 3840, Loss: 0.982156\n",
      "Iteration 3920, Loss: 0.978759\n"
     ]
    }
   ],
   "source": [
    "## Training loop for updater and predictor\n",
    "iters = 4000\n",
    "alpha = 0.008\n",
    "\n",
    "for c in range(iters):\n",
    "    # Zero updater gradients\n",
    "    for p in updaterparams:\n",
    "        if p.grad is not None:\n",
    "            p.grad.zero_()\n",
    "\n",
    "    # Zero predictor gradients\n",
    "    if W_p_t_1.grad is not None:\n",
    "        W_p_t_1.grad.zero_()\n",
    "\n",
    "    W_p_t_detached = W_p_t_1.detach()\n",
    "    flat_W_p_t_detached = W_p_t_detached.view(-1)\n",
    "    x = torch.tanh(flat_W_p_t_detached @ W_u1)\n",
    "    flat_W_p_t_1 = torch.tanh(x @ W_u2)\n",
    "    W_p_t_1 = flat_W_p_t_1.view(784, 10)\n",
    "\n",
    "    Y_p_t_1 = torch.tanh(X_train @ W_p_t_1)\n",
    "    loss = f.cross_entropy(Y_p_t_1, Y_train)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    for p in updaterparams:\n",
    "        p.data -= alpha * p.grad\n",
    "\n",
    "    if c % (iters // 50) == 0:\n",
    "        print(f\"Iteration {c:4d}, Loss: {loss.item():.6f}\")\n",
    "        losses.append(loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7209328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# plt.figure(figsize=(8, 4))\n",
    "# plt.plot(losses)\n",
    "# plt.xlabel('Iteration')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Training Loss')\n",
    "\n",
    "# # Get current notebook filename and save as PNG\n",
    "# notebook_path = 'model_descent_weight_enc_dec_compression_3920_4000it'\n",
    "# filename = os.path.splitext(notebook_path)[0] + '.png'\n",
    "# plt.savefig(filename)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eadd2ac6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eadd2ac6",
    "outputId": "3c2a48ec-2b57-421b-f6b6-6bb794d0ba23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 90.755 | test accuracy: 90.52\n",
      "train loss: 0.9750391244888306 | test loss: 0.9913414120674133\n"
     ]
    }
   ],
   "source": [
    "def accuracy(X, Y):\n",
    "    x = torch.tanh(X @ W_p_t_1)\n",
    "    probs = f.softmax(x, 1)\n",
    "    answers = x.argmax(1)\n",
    "    c = 0\n",
    "    for a, y in zip(answers, Y):\n",
    "        if a==y: c+=1\n",
    "    return c / answers.shape[0] * 100\n",
    "\n",
    "def loss(X, Y):\n",
    "    x = torch.tanh(X @ W_p_t_1)\n",
    "    return f.cross_entropy(x, Y)\n",
    "\n",
    "print(f\"train accuracy: {accuracy(X_train, Y_train)} | test accuracy: {accuracy(X_test, Y_test)}\")\n",
    "print(f\"train loss: {loss(X_train, Y_train)} | test loss: {loss(X_test, Y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b349e18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 60000\n",
    "# #loss1iters = []\n",
    "\n",
    "\n",
    "# for midf in range(24, 34):\n",
    "#     mid_dims = (midf+1) * 196\n",
    "#     W_u1 = (torch.randn(7840, mid_dims) * (5/3) / (7840**0.5)).to(device)\n",
    "#     W_u2 = (torch.randn(mid_dims, 7840) * (5/3) / (mid_dims**0.5)).to(device)\n",
    "#     W_p_t_1 = (torch.randn(784, 10) * (5/3) / (784**0.5)).to(device)\n",
    "\n",
    "#     W_u1.requires_grad = True\n",
    "#     W_u2.requires_grad = True\n",
    "#     W_p_t_1.requires_grad = False\n",
    "\n",
    "#     updaterparams = [W_u1, W_u2]\n",
    "\n",
    "#     print(f\"Updater parameters: {W_u1.numel() + W_u2.numel()}, Predictor parameters: {W_p_t_1.numel()}\")\n",
    "\n",
    "#     losses = []\n",
    "\n",
    "#     ## Training loop for updater and predictor\n",
    "#     alpha = 0.008\n",
    "\n",
    "#     c = -1\n",
    "#     while True:\n",
    "#         c += 1\n",
    "\n",
    "#         # Zero updater gradients\n",
    "#         for p in updaterparams:\n",
    "#             if p.grad is not None:\n",
    "#                 p.grad.zero_()\n",
    "\n",
    "#         # Zero predictor gradients\n",
    "#         if W_p_t_1.grad is not None:\n",
    "#             W_p_t_1.grad.zero_()\n",
    "\n",
    "#         W_p_t_detached = W_p_t_1.detach()\n",
    "#         flat_W_p_t_detached = W_p_t_detached.view(-1)\n",
    "#         x = torch.tanh(flat_W_p_t_detached @ W_u1)\n",
    "#         flat_W_p_t_1 = torch.tanh(x @ W_u2)\n",
    "#         W_p_t_1 = flat_W_p_t_1.view(784, 10)\n",
    "\n",
    "#         Y_p_t_1 = torch.tanh(X_train @ W_p_t_1)\n",
    "#         loss = f.cross_entropy(Y_p_t_1, Y_train)\n",
    "\n",
    "#         loss.backward()\n",
    "\n",
    "#         for p in updaterparams:\n",
    "#             p.data -= alpha * p.grad\n",
    "\n",
    "#         if c>6000: \n",
    "#             print(\"Couldn't find a solution in 6000 iterations\")\n",
    "#             break\n",
    "#         if loss.item() < 1.05:\n",
    "#             print(\"Found a solution in\", c, \"iterations with mid_dims =\", mid_dims)\n",
    "#             loss1iters.append([mid_dims, c])\n",
    "#             break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "701721ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss1iters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Plotting loss1iters with (midf+1)*392 on x-axis and c on y-axis\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mloss1iters\u001b[49m:\n\u001b[1;32m      5\u001b[0m     loss1iters_arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(loss1iters)\n\u001b[1;32m      6\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m4\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss1iters' is not defined"
     ]
    }
   ],
   "source": [
    "# Plotting loss1iters with (midf+1)*392 on x-axis and c on y-axis\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if loss1iters:\n",
    "    loss1iters_arr = np.array(loss1iters)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(loss1iters_arr[:,0], loss1iters_arr[:,1], marker='o')\n",
    "    plt.xlabel('Mid Dims')\n",
    "    plt.ylabel('Iterations to reach loss < 1.0')\n",
    "    plt.title('Iterations vs. Hidden Layer Size')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"loss1iters is empty.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
